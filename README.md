# Rapport - Mise en place d‚Äôune architecture et d‚Äôune infrastructure r√©seau avec AWS

**√âQUIPE** :

Pierre LEOCADIE
Jordan BAUMARD
Charles HURST

**Groupe 209**

![Untitled](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Untitled.png)

# Sommaire

- Introduction
- Sch√©mas
- √âtape 1 : Infrastructure provisioning
    - Description de l‚Äôinfrastructure
    - Code source Terraform de l‚Äôinfrastructure
    - D√©ploiement de l‚Äôinfrastructure avec Terraform
    - Configuration du nom de domaine
- √âtape 2 : Configuration de l‚Äôinfrastructure
    - Description
        - L‚Äôutilisation de Docker
        - NGINX Reverse Proxy
        - GoAcces
        - Serveur VPN WireGuard
        - Interface web d‚Äôadministration pour WireGuard
        - Portainer
        - Serveur DNS Pihole
        - Les deux autres serveurs : **`cloudlab_public_app_projects_server`** et **`cloudlab_internal_server_2`**
        - R√©sum√© de l‚Äôinfrastructure et de sa configuration
    - Code source Ansible de la configuration
    - Configuration avec Ansible et Portainer
        - Tester la connectivit√©
        - Ex√©cution des playbooks Ansible
        - Configuration de Portainer
        - D√©ploiement des autres applications avec Portainer
- √âtape 3 : Modification de l‚Äôinfrastructure provisioning
- Conclusion

# Introduction

Dans le cadre de notre projet, nous avons entrepris la conception et la mise en place d'une architecture et d'une infrastructure r√©seau sur AWS (Amazon Web Services). Ce rapport vise √† pr√©senter en d√©tail notre approche et les √©tapes cl√©s de d√©ploiement de cette infrastructure, en fournissant des explications simples et claires accessibles √† tous.

L'objectif principal de notre projet √©tait de cr√©er une infrastructure solide, s√©curis√©e et scalable, permettant de d√©ployer et de g√©rer efficacement diff√©rents services au sein d‚Äôun environnement cloud. Pour atteindre cet objectif, nous avons utilis√© des technologies telles que Docker, Terraform, Ansible, ainsi que des services AWS tels que VPC (Virtual Private Cloud), EC2 (Elastic Compute Cloud), Route 53 et bien d'autres.

Ce rapport mettra l'accent sur les principales √©tapes de d√©ploiement de l'architecture et de l'infrastructure r√©seau, en fournissant des explications d√©taill√©es et des instructions simples pour permettre une compr√©hension compl√®te, m√™me pour les personnes non familiaris√©es avec ces technologies.

Le rapport commencera par une description g√©n√©rale de l'architecture, en expliquant les diff√©rents composants et leur r√¥le dans notre infrastructure. Nous d√©taillerons ensuite les √©tapes de d√©ploiement, en fournissant des instructions claires sur la configuration des diff√©rents services tels que NGINX Proxy Manager, WireGuard, Pihole, et GoAccess.

Nous mettrons √©galement l'accent sur les aspects de s√©curit√©, en expliquant les mesures prises pour prot√©ger notre infrastructure, y compris la configuration des groupes de s√©curit√©, l'application d'access lists et la restriction des acc√®s non autoris√©s.

Ce rapport vise √† fournir une vision globale de notre projet, en pr√©sentant les √©tapes cl√©s de d√©ploiement de l'architecture et de l'infrastructure r√©seau AWS. Nous esp√©rons que les informations pr√©sent√©es ici permettront une compr√©hension approfondie de notre approche et encourageront d'autres √©tudiants √† explorer les avantages et les possibilit√©s offerts par les services cloud et les technologies modernes de d√©ploiement d'infrastructures.

# Sch√©mas

![AWSInfra-vFinal.drawio.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/AWSInfra-vFinal.drawio.png)

![IPs.drawio.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/IPs.drawio.png)

# √âtape 1 : Infrastructure provisioning

## Description de l‚Äôinfrastructure

- VPC (Virtual Private Cloud) : Nous avons cr√©√© un VPC appel√© **`cloudlab_vpc`** avec un bloc CIDR de **`10.0.0.0/16`**. Le VPC agit comme un r√©seau isol√© dans le cloud d'AWS.
- Internet Gateway : Nous avons attach√© un Internet Gateway nomm√© **`cloudlab_vpc_igw`** au VPC **`cloudlab_vpc`**. Cela permet aux ressources situ√©es dans le VPC d'acc√©der √† Internet.
- Table de routage : Nous avons cr√©√© une table de routage publique appel√©e **`cloudlab_public_route_table`** avec un bloc CIDR source de **`0.0.0.0/0`** et associ√©e √† l'Internet Gateway **`cloudlab_vpc_igw`**. Cette table de routage est utilis√©e pour diriger le trafic Internet des ressources publiques du VPC.
- Sous-r√©seau 1 : Nous avons cr√©√© un sous-r√©seau public nomm√© **`cloudlab_vpc_public_facing_subnet_3a1`** avec un bloc CIDR de **`10.0.1.0/24`**. Ce sous-r√©seau est situ√© dans la zone de disponibilit√© **`eu-west-3a`** (PARIS zone A) et est destin√© √† h√©berger des instances EC2 qui sont expos√©es √† Internet (directement dans le cas du serveur **`cloudlab_public_facing_entrypoint`** et indirectement dans le cas du serveur **`cloudlab_public_app_projects_server`**). Les adresses IP publiques sont attribu√©es automatiquement aux instances EC2 de ce sous-r√©seau. Il est associ√© √† la table de routage publique **`cloudlab_public_route_table`**.
- Groupe de s√©curit√© du sous-r√©seau 1 : Nous avons cr√©√© un groupe de s√©curit√© appel√© **`cloudlab_public_facing_subnet_3a1_sg`** associ√© au VPC **`cloudlab_vpc`**. Ce groupe de s√©curit√© autorise les connexions TCP et UDP sur tous les ports (0-65535) √† partir des blocs CIDR sources **`10.0.0.0/16`**, **`172.0.0.0/8`**et **`0.0.0.0/0`**. Il est utilis√© pour contr√¥ler les acc√®s r√©seau aux instances EC2 du sous-r√©seau public.
- Instances EC2 du sous-r√©seau 1 : Nous avons cr√©√© deux instances EC2 dans le sous-r√©seau **`cloudlab_vpc_public_facing_subnet_3a1`**. Les instances sont nomm√©es **`cloudlab_public_facing_entrypoint`** avec l'adresse IP priv√©e **`10.0.1.11`** et **`cloudlab_public_app_projects_server`** avec l'adresse IP priv√©e **`10.0.1.12`**. Les instances utilisent l'AMI Debian 11 architecture ARM64 et le type d'instance **`t4g.micro`**. Elles sont associ√©es au sous-r√©seau **`cloudlab_vpc_public_facing_subnet_3a1`** et au groupe de s√©curit√© **`cloudlab_public_facing_subnet_3a1_sg`**.
- Elastic IP : Nous avons associ√© une adresse IP √©lastique (Elastic IP) nomm√©e **`public_facing_entrypoint_eip`** √† l'instance EC2 **`cloudlab_public_facing_entrypoint`**. L'Elastic IP permet d'associer une adresse IP statique √† l'instance EC2, garantissant ainsi que l'adresse IP ne change pas lors du red√©marrage de l'instance.
- Sous-r√©seau 2 : Nous avons cr√©√© un sous-r√©seau interne nomm√© **`cloudlab_vpc_internal_subnet_3b1`** avec un bloc CIDR de **`10.0.2.0/24`**. Ce sous-r√©seau est situ√© dans la zone de disponibilit√© **`eu-west-3b`** (PARIS zone B) et est destin√© √† h√©berger des instances EC2 qui ne sont accessibles que depuis le r√©seau interne. Les adresses IP publiques sont attribu√©es automatiquement aux instances EC2 de ce sous-r√©seau. Il est associ√© √† la table de routage publique **`cloudlab_public_route_table`**.
- Groupe de s√©curit√© du sous-r√©seau 2 : Nous avons cr√©√© un groupe de s√©curit√© appel√© **`cloudlab_internal_subnet_3b1_sg`** associ√© au VPC **`cloudlab_vpc`**. Ce groupe de s√©curit√© autorise les connexions TCP et UDP sur tous les ports (0-65535) √† partir des blocs CIDR sources **`10.0.0.0/16`**, **`172.0.0.0/8`** et **`0.0.0.0/0`**. Il est utilis√© pour contr√¥ler les acc√®s r√©seau aux instances EC2 du sous-r√©seau interne.
- Instances EC2 du sous-r√©seau 2 : Nous avons cr√©√© deux instances EC2 dans le sous-r√©seau **`cloudlab_vpc_internal_subnet_3b1`**. Les instances sont nomm√©es **`cloudlab_internal_server_1`** avec l'adresse IP priv√©e **`10.0.2.11`** et **`cloudlab_internal_server_2`** avec l'adresse IP priv√©e **`10.0.2.12`**. Les instances utilisent l'AMI Debian 11 architecture ARM64 et le type d'instance **`t4g.micro`**. Elles sont associ√©es au sous-r√©seau **`cloudlab_vpc_internal_subnet_3b1`** et au groupe de s√©curit√© **`cloudlab_internal_subnet_3b1_sg`**.
- DNS - Route53 : Nous avons configur√© la zone publique **`deletesystem32.fr`** dans Route53. Les serveurs de noms associ√©s sont **`ns-116.awsdns-14.com`**, **`ns-1352.awsdns-41.org`**, **`ns-1873.awsdns-42.co.uk`** et **`ns-958.awsdns-55.net`**. Les enregistrements de type A sont utilis√©s pour associer des adresses IP publiques √† des noms de domaine, tels que **`deletesystem32.fr`**, **`proxymanager.deletesystem32.fr`**, **`goaccess.deletesystem32.fr`**, etc. L'adresse IP publique Elastic IP de l'instance **`cloudlab_public_facing_entrypoint`** est associ√©e aux enregistrements de type A.

## Code source Terraform de l‚Äôinfrastructure

Dans le cadre de notre projet, nous avons utilis√© Terraform, une technologie connue sous le nom d'**Infrastructure as Code** (IaC), pour d√©ployer et g√©rer notre infrastructure AWS. L'Infrastructure as Code consiste √† d√©crire et √† g√©rer l'infrastructure de mani√®re programmable, en utilisant du code source plut√¥t que des interfaces graphiques ou des actions manuelles.

<aside>
‚ö†Ô∏è √âtant donn√© que le code source Terraform de notre infrastructure ne comprend pas la cr√©ation d'une paire de cl√©s SSH, il est n√©cessaire d'ajouter vous-m√™me le code Terraform qui le permet, ou de proc√©der √† la cr√©ation d'une paire de cl√©s SSH via la console AWS. Voici les √©tapes pour cr√©er une paire de cl√©s SSH via la console AWS :

1. Connectez-vous √† votre compte AWS et acc√©dez √† la console AWS.
2. Recherchez le service "EC2" dans la barre de navigation √† gauche et cliquez dessus. EC2 est le service d'Amazon qui vous permet de cr√©er et g√©rer des instances de machines virtuelles.
3. Dans la section "R√©seau et s√©curit√©" de la barre de navigation √† gauche de la console EC2, recherchez l'option "Paires de cl√©s" et cliquez dessus.
4. Sur la page "Paires de cl√©s", cliquez sur le bouton "Cr√©er une paire de cl√©s".
5. Dans la fen√™tre qui appara√Æt, donnez un nom √† votre paire de cl√©s pour faciliter son identification. Assurez-vous de s√©lectionner le format de fichier ".pem" pour votre cl√©.
6. Cliquez sur le bouton "Cr√©er une paire de cl√©s" pour finaliser la cr√©ation de la paire. La paire de cl√©s sera alors g√©n√©r√©e et automatiquement t√©l√©charg√©e sur votre machine.

Apr√®s avoir cr√©√© la paire de cl√©s SSH, vous pouvez l'utiliser pour acc√©der √† vos instances EC2 dans votre infrastructure AWS. Si vous utilisez Terraform pour le d√©ploiement, vous pouvez ajouter le code Terraform appropri√© pour sp√©cifier l'utilisation de la paire de cl√©s dans votre configuration.

Il est important de noter que les cl√©s SSH sont utilis√©es pour s√©curiser les connexions √† vos instances EC2 et pour vous permettre de vous y connecter de mani√®re s√©curis√©e. Assurez-vous de stocker votre cl√© priv√©e en lieu s√ªr et de ne pas la partager avec des personnes non autoris√©es.

Si vous avez des questions ou des difficult√©s lors de la cr√©ation de la paire de cl√©s SSH, n'h√©sitez pas √† consulter la documentation AWS pour obtenir des instructions pr√©cises.

</aside>

Notre code source Terraform est organis√© dans le dossier **`üìÅ terraform/`** et se compose de cinq fichiers principaux :

- **`üìÑ¬†main.tf`** : Ce fichier contient la configuration principale de notre infrastructure. Il permet de d√©finir les ressources et les param√®tres n√©cessaires √† la cr√©ation et √† la gestion de notre infrastructure AWS. Dans ce fichier, vous pouvez modifier la variable locale **`key_name`** pour sp√©cifier le nom de votre propre cl√© SSH.
    
    ```java
    key_name = "HomelabInfra"
    ```
    
- **`üìÑ¬†variables.tf`** : Ce fichier contient les variables utilis√©es dans notre configuration Terraform. Il permet de personnaliser certains param√®tres de d√©ploiement, tels que le chemin d'acc√®s √† votre cl√© SSH. Vous pouvez modifier la variable **`default`** pour sp√©cifier le chemin d'acc√®s √† votre propre cl√© SSH.
    
    ```java
    default     = "../ssh_keys/HomelabInfra.pem"
    ```
    
- **`üìÑ¬†3a1_subnet_configuration.tf`** et **`üìÑ¬†3b1_subnet_configuration.tf`** : Ces fichiers d√©crivent la configuration des sous-r√©seaux et de leurs ressources (instances EC2) dans les zones de disponibilit√© **`eu-west-3a`** (PARIS zone A) et **`eu-west-3b`** (PARIS zone B). Ils sp√©cifient le bloc CIDR, la description et les autres param√®tres n√©cessaires √† la cr√©ation des sous-r√©seaux et leurs ressources.
- **`üìÑ¬†domain.tf`** : Ce fichier est utilis√© pour configurer notre nom de domaine dans Route53. Vous pouvez le modifier en fonction de votre propre nom de domaine en vous basant sur l'exemple fourni dans le fichier.

En utilisant Terraform, nous avons pu d√©ployer notre infrastructure AWS de mani√®re coh√©rente, reproductible et √©volutive. Le code source Terraform nous permet de versionner notre infrastructure, de la partager et de la g√©rer de mani√®re efficace en utilisant les principes de l'Infrastructure as Code.

## D√©ploiement de l‚Äôinfrastructure avec Terraform

<aside>
‚ö†Ô∏è Avant de commencer, assurez-vous d'avoir install√© AWS CLI et Terraform sur votre machine. De plus, assurez-vous d'√™tre connect√© √† votre compte avec AWS CLI.

</aside>

Pour d√©ployer notre infrastructure √† l'aide de Terraform, suivez ces √©tapes simples :

1. Ouvrez votre terminal et rendez-vous dans le dossier **`üìÅ terraform/`** de notre projet.
2. Ex√©cutez la commande suivante pour initialiser Terraform et configurer l'environnement de travail :
    
    ```bash
    terraform init
    ```
    
    Cette commande va t√©l√©charger les plugins et les d√©pendances n√©cessaires pour le d√©ploiement de l'infrastructure.
    
3. Ensuite, ex√©cutez la commande suivante pour afficher le plan d'ex√©cution de Terraform :
    
    ```bash
    terraform plan
    ```
    
    Cette commande va analyser notre code Terraform et afficher les actions que Terraform va effectuer pour d√©ployer notre infrastructure. Vous pourrez voir les ressources qui seront cr√©√©es, mises √† jour ou supprim√©es.
    
4. Apr√®s avoir v√©rifi√© le plan d'ex√©cution et si tout semble correct, vous pouvez proc√©der au d√©ploiement de l'infrastructure en ex√©cutant la commande suivante :
    
    ```bash
    terraform apply
    ```
    
    Cette commande va ex√©cuter les actions d√©crites dans le plan pr√©c√©dent. Terraform va cr√©er et configurer les ressources sp√©cifi√©es dans notre code Terraform, telles que les VPC, les sous-r√©seaux, les groupes de s√©curit√©, etc.
    
    Assurez-vous de v√©rifier et de confirmer les actions avant de proc√©der, car cela peut entra√Æner des modifications dans votre compte AWS.
    

En suivant ces √©tapes simples, vous pourrez d√©ployer notre infrastructure r√©seau AWS √† l'aide de Terraform de mani√®re coh√©rente et reproductible.

## Configuration du nom de domaine

Afin de faire fonctionner votre nom de domaine avec AWS, vous devez le configurer aupr√®s de votre fournisseur de nom de domaine. Dans mon cas, j'ai achet√© mon nom de domaine chez OVH. Je vais donc acc√©der √† mon espace client OVH pour effectuer les modifications n√©cessaires.

Dans l'espace client OVH, je vais chercher l'option permettant de modifier les serveurs de noms (name servers) associ√©s √† mon nom de domaine. Une fois que j'aurai trouv√© cette option, je vais proc√©der au remplacement des serveurs de noms actuels par ceux fournis par AWS.

En utilisant les informations fournies par AWS, je vais saisir les nouveaux serveurs de noms dans les param√®tres de mon nom de domaine chez OVH. Cela permettra de diriger les requ√™tes DNS vers la configuration DNS que nous avons d√©ploy√©e sur AWS.

Une fois les modifications enregistr√©es, je vais attendre un certain temps pour que les changements de serveurs de noms se propagent √† travers Internet. Ce processus de propagation peut prendre plus ou moins de temps, en fonction des param√®tres de votre fournisseur de nom de domaine.

Une fois que les serveurs de noms sont correctement configur√©s et que la propagation DNS est termin√©e, votre nom de domaine sera fonctionnel avec votre infrastructure AWS. Vous pourrez utiliser votre nom de domaine pour acc√©der √† vos ressources h√©berg√©es sur AWS, telles que des sites web, des applications, des services, etc.

Il est important de noter que la proc√©dure de configuration des serveurs de noms peut varier en fonction de votre fournisseur de nom de domaine. Assurez-vous de consulter la documentation ou de contacter le support de votre fournisseur pour obtenir des instructions pr√©cises sur la configuration des serveurs de noms.

Pour les utilisateurs d'OVH, voici les √©tapes √† suivre pour configurer votre nom de domaine avec votre infrastructure r√©seau AWS :

- Cliquez sur **Web Cloud** dans la barre de navigation,
- Puis cliquez sur **Noms de domaine** dans la barre de navigation √† gauche,
- Rendez-vous dans l‚Äôonglet **Serveurs DNS**,
- Cliquez sur **Modifier les serveurs DNS**.

![OVH 2.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/OVH_2.png)

Lorsque vous utilisez Terraform pour d√©ployer votre infrastructure r√©seau sur AWS, il est important de conna√Ætre les noms des serveurs DNS associ√©s √† votre configuration. Ces informations vous seront n√©cessaires pour configurer correctement votre nom de domaine aupr√®s de votre fournisseur de noms de domaine.

Pour obtenir les noms des serveurs DNS √† utiliser, vous avez deux options :

1. Outputs de la commande **`terraform apply`** : Apr√®s avoir ex√©cut√© la commande **`terraform apply`** dans votre terminal pour d√©ployer votre infrastructure, vous pouvez consulter les **`Outputs`** g√©n√©r√©s par Terraform. Les **`Outputs`** sont des valeurs que Terraform affiche √† la fin du d√©ploiement et qui sont utiles pour r√©cup√©rer des informations importantes. Parmi ces informations, vous trouverez les noms des serveurs DNS associ√©s √† votre infrastructure. Notez ces noms pour les utiliser lors de la configuration de votre nom de domaine.
2. Commande **`terraform refresh`** : Si vous avez d√©j√† d√©ploy√© votre infrastructure avec Terraform mais que vous n'avez pas acc√®s aux **`Outputs`** ou que vous souhaitez simplement actualiser les informations, vous pouvez ex√©cuter la commande **`terraform refresh`** dans votre terminal. Cette commande permet √† Terraform de r√©cup√©rer les derni√®res informations de l'√©tat actuel de votre infrastructure sur AWS. Une fois la commande ex√©cut√©e, vous pouvez rechercher les noms des serveurs DNS dans les r√©sultats affich√©s par Terraform.

En utilisant ces deux m√©thodes, vous serez en mesure de trouver les noms des serveurs DNS n√©cessaires pour configurer correctement votre nom de domaine. Assurez-vous de noter ces informations et de les utiliser lors de la configuration aupr√®s de votre fournisseur de noms de domaine.

Si vous avez des difficult√©s √† obtenir les noms des serveurs DNS, n'h√©sitez pas √† demander de l'aide √† votre √©quipe de d√©veloppement ou √† consulter la documentation de Terraform pour obtenir des instructions pr√©cises sur la r√©cup√©ration des **`Outputs`** ou l'utilisation de la commande **`terraform refresh`**.

![Capture d‚ÄôeÃÅcran 2023-06-17 aÃÄ 13.31.47.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-17_a_13.31.47.png)

Vous pouvez √©galement les retrouver avec la console AWS dans **Route53 > Zones h√©berg√©es (barre de navigation √† gauche) > Cliquez sur la zone de votre nom de domaine > Avec l‚Äôenregistrement de type NS**.

![Capture d‚ÄôeÃÅcran 2023-06-17 aÃÄ 13.37.05.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-17_a_13.37.05.png)

Dans mon cas, les serveurs de noms (name servers) √† utiliser pour ma configuration AWS sont les suivants :

- **`ns-1031.awsdns-00.org`**
- **`ns-1606.awsdns-08.co.uk`**
- **`ns-300.awsdns-37.com`**
- **`ns-830.awsdns-39.net`**

Modifez les Serveurs DNS sur votre espace OVH, appliquez les modifications et vous devriez avoir ce qui suit :

![Capture d‚ÄôeÃÅcran 2023-06-17 aÃÄ 13.04.58.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-17_a_13.04.58.png)

Dans votre cas les Serveurs DNS en cours de suppression seront ceux de OVH.

<aside>
‚ÑπÔ∏è Il est important de noter que les √©tapes sp√©cifiques pour modifier les serveurs de noms peuvent varier en fonction de l'interface client OVH. Si vous rencontrez des difficult√©s pour effectuer ces modifications, je vous recommande de consulter la documentation d'OVH ou de contacter leur support technique pour obtenir une assistance suppl√©mentaire.

</aside>

# √âtape 2 : Configuration de l‚Äôinfrastructure

## Description

### L‚Äôutilisation de Docker

Pour commencer, dans notre projet, tous les services seront d√©ploy√©s √† l'aide de Docker, qui est une technologie de conteneurisation. Cela signifie que chaque service sera encapsul√© dans un conteneur isol√©, garantissant ainsi une gestion simplifi√©e et une portabilit√© √©lev√©e des applications.

L'utilisation de Docker dans notre projet pr√©sente plusieurs avantages significatifs pour notre infrastructure bas√©e sur AWS. Voici quelques-uns des avantages cl√©s d'avoir une infrastructure bas√©e sur Docker :

1. **Isolation et portabilit√©** : Docker permet d'encapsuler chaque service dans un conteneur isol√©, ce qui garantit que chaque application et ses d√©pendances sont parfaitement s√©par√©es les unes des autres. Cette isolation garantit la stabilit√© et la fiabilit√© de chaque service, tout en facilitant la gestion des applications. De plus, les conteneurs Docker sont portables, ce qui signifie qu'ils peuvent √™tre ex√©cut√©s sur n'importe quel environnement compatible Docker, que ce soit sur une machine locale, sur un serveur distant ou dans le cloud.
2. **Gestion simplifi√©e** : Docker fournit des outils puissants pour cr√©er, d√©ployer, g√©rer et surveiller les conteneurs. L'utilisation de Docker facilite la gestion de l'infrastructure en automatisant les t√¢ches de d√©ploiement et de configuration, r√©duisant ainsi la charge de travail administrative. De plus, Docker permet de mettre √† jour et de faire √©voluer facilement les applications, en minimisant les temps d'arr√™t et les interruptions de service.
3. **Scalabilit√© et flexibilit√©** : Gr√¢ce √† Docker, il est facile de mettre en place une infrastructure scalable et flexible. En utilisant des outils tels que Docker Swarm ou Kubernetes, il est possible d‚Äôorchestrer la gestion des conteneurs et assurer une distribution √©quilibr√©e de la charge de travail sur plusieurs n≈ìuds. Cette capacit√© de mise √† l'√©chelle horizontale permet d'adapter rapidement l'infrastructure aux besoins changeants, en ajoutant ou en supprimant facilement des instances de conteneurs selon les exigences de charge de travail.
4. **Reproductibilit√© et facilit√© de d√©ploiement** : Docker permet de cr√©er des images d'applications reproductibles, qui incluent tous les √©l√©ments n√©cessaires √† l'ex√©cution de l'application. Ces images peuvent √™tre partag√©es et d√©ploy√©es sur n'importe quel environnement compatible Docker, ce qui facilite grandement le d√©ploiement des applications sur diff√©rents serveurs ou plateformes. Cela simplifie √©galement le processus de mise en production, en r√©duisant les probl√®mes de compatibilit√© et en garantissant une exp√©rience de d√©ploiement coh√©rente.

En r√©sum√©, l'utilisation de Docker dans notre infrastructure bas√©e sur AWS offre une isolation efficace, une gestion simplifi√©e, une √©volutivit√© flexible et une reproductibilit√© √©lev√©e. Ces avantages permettent d'optimiser l'administration, de faciliter le d√©ploiement des applications et de garantir une infrastructure stable et performante.

### NGINX Reverse Proxy

Dans la configurition de notre infrastructure, nous allons installer un reverse proxy NGINX avec NGINX Proxy Manager sur l'instance EC2 appel√©e **`cloudlab_public_facing_entrypoint`**. Ce service agira comme le routeur et le point d'entr√©e de notre infrastructure. Sa disponibilit√© est cruciale, car en cas de panne de ce service, cela peut causer des probl√®mes majeurs. L'utilisation d'un reverse proxy pr√©sente plusieurs avantages :

1. **Gestion du trafic** : Le reverse proxy, ici NGINX, permettra de g√©rer et de diriger efficacement le trafic entrant vers les services appropri√©s. Il agira comme une passerelle pour rediriger les requ√™tes des clients vers les bons services en fonction des r√®gles de routage et de configuration sp√©cifi√©es.
2. **S√©curit√©** : Le reverse proxy peut agir comme une barri√®re de s√©curit√© en filtrant le trafic et en prot√©geant les services backend des attaques potentielles. Il peut √©galement prendre en charge des fonctionnalit√©s de s√©curit√© suppl√©mentaires, telles que le chiffrement SSL/TLS pour s√©curiser les communications avec les clients.
3. **Gestion des certificats SSL** : Le reverse proxy NGINX Proxy Manager facilite la gestion des certificats SSL/TLS pour les domaines personnalis√©s. Il peut g√©n√©rer automatiquement et renouveler les certificats SSL pour chaque domaine, assurant ainsi une connexion s√©curis√©e entre les clients et les services.

![Interface web NGINX Proxy Manager - Ici il s‚Äôagit des r√®gles de redirections vers les bons services, sur les bons ports, sur les bonnes machines](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-25_a_23.13.14.png)

Interface web NGINX Proxy Manager - Ici il s‚Äôagit des r√®gles de redirections vers les bons services, sur les bons ports, sur les bonnes machines

### GoAccess

Pour pouvoir visualiser les logs de notre reverse proxy et obtenir des statistiques sur les requ√™tes qui passent par celui-ci, nous allons utiliser une application appel√©e GoAccess.

GoAccess est un outil de visualisation de logs qui nous permet d'analyser et de pr√©senter les donn√©es de nos logs sous forme de tableaux et de graphiques interactifs. Cela nous offre une meilleure compr√©hension du trafic √† travers notre reverse proxy NGINX.

L'installation et l'utilisation de GoAccess sont relativement simples. Nous allons l'installer (toujours avec Docker bien s√ªr) sur notre instance EC2 qui h√©berge le reverse proxy NGINX. Une fois install√©, GoAccess pourra traiter les logs g√©n√©r√©s par NGINX et afficher les informations pertinentes, telles que les adresses IP des clients, les codes de r√©ponse HTTP, les pages les plus consult√©es etc.

Gr√¢ce √† cette visualisation des logs, nous pourrons :

1. **Suivre les performances** : GoAccess nous permettra de surveiller les performances de notre reverse proxy. Nous pourrons ainsi prendre des mesures pour optimiser notre infrastructure et am√©liorer l'exp√©rience utilisateur.
2. **Identifier les comportements suspects** : En analysant les logs avec GoAccess, nous pourrons rep√©rer les activit√©s anormales ou les tentatives d'acc√®s non autoris√©es √† nos services. Cela nous permettra de r√©agir rapidement et de renforcer les mesures de s√©curit√© si n√©cessaire.
3. **Analyser les tendances de trafic** : GoAccess nous offre une vue d'ensemble du trafic, nous permettant d'identifier les p√©riodes de pointe, les tendances de navigation et les pages les plus consult√©es. Ces informations peuvent √™tre utilis√©es pour optimiser notre infrastructure, planifier les ressources et am√©liorer notre strat√©gie de contenu.

![Interface web GoAccess pour visualiser les logs et avoir quelques statistiques sur les requ√™tes qui sont effectu√©es au travers du reverse proxy NGINX](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-25_a_23.17.37.png)

Interface web GoAccess pour visualiser les logs et avoir quelques statistiques sur les requ√™tes qui sont effectu√©es au travers du reverse proxy NGINX

### Serveur VPN WireGuard

Nous √©galement allons installer un serveur VPN WireGuard sur cette m√™me instance EC2. Le VPN, ou R√©seau Priv√© Virtuel, nous permettra d'acc√©der aux autres machines et services de notre infrastructure r√©seau, m√™me si nous sommes √† l'ext√©rieur du r√©seau du VPC et qu'ils ne sont pas directement accessibles depuis Internet.

L'utilisation d'un VPN pr√©sente plusieurs avantages :

1. **Acc√®s s√©curis√© aux ressources internes** : En utilisant le VPN WireGuard, nous pourrons √©tablir une connexion s√©curis√©e et chiffr√©e avec notre r√©seau priv√©. Cela nous permettra d'acc√©der aux machines et services internes de mani√®re s√©curis√©e, m√™me depuis des emplacements distants, tels que des bureaux distants ou des connexions Wi-Fi publiques.
2. **Protection des donn√©es** : Le VPN chiffre les donn√©es transitant entre notre appareil et le r√©seau interne, ce qui garantit que nos informations sensibles sont prot√©g√©es contre les interceptions et les attaques potentielles. Cela est particuli√®rement important lorsque nous acc√©dons √† des ressources sensibles ou confidentielles.
3. **Contournement des restrictions de r√©seau** : Si certaines ressources internes ne sont pas directement accessibles depuis Internet, le VPN nous permettra de contourner ces restrictions en cr√©ant un tunnel s√©curis√© qui nous connecte au r√©seau interne. Cela nous permettra d'acc√©der aux services et aux donn√©es internes comme si nous √©tions physiquement connect√©s au r√©seau local.
4. **Filtrage bas√© sur l'adresse IP** : En combinant le VPN avec notre reverse proxy, nous pourrons √©tablir des filtres bas√©s sur l'adresse IP. Cela signifie que nous pouvons contr√¥ler l'acc√®s aux services en fonction de l'adresse IP de l'utilisateur, ajoutant une couche de s√©curit√© suppl√©mentaire √† notre infrastructure.

En r√©sum√©, l'installation d'un serveur VPN WireGuard sur notre instance EC2, utilisant Docker, nous permettra d'acc√©der de mani√®re s√©curis√©e aux ressources internes de notre infrastructure r√©seau AWS, m√™me depuis des emplacements distants. Cela renforce la s√©curit√© de nos communications, nous permet de contourner les restrictions r√©seau et offre une flexibilit√© accrue pour l'acc√®s aux services internes.

### Interface web d‚Äôadministration pour WireGuard

Dans le but de simplifier l'administration du serveur VPN WireGuard, nous allons utiliser une interface web appel√©e WireGuard-UI. Cette interface web conviviale sera install√©e sur la m√™me instance EC2 que celle utilis√©e pour notre serveur VPN WireGuard.

WireGuard-UI est une application sp√©cialement con√ßue pour simplifier la gestion et la configuration des connexions VPN WireGuard. Elle offre une interface utilisateur conviviale qui permet de g√©rer facilement les clients VPN et les param√®tres de configuration.

Pour installer WireGuard-UI, nous utiliserons Docker Compose, une technologie qui facilite le d√©ploiement et la gestion d'applications conteneuris√©es. Avec Docker Compose, nous pourrons rapidement d√©ployer l'interface web WireGuard-UI en m√™me temps que le serveur VPN WireGuard en quelques √©tapes simples.

Une fois WireGuard-UI install√©, nous pourrons profiter des fonctionnalit√©s suivantes :

1. **Gestion des clients VPN** : WireGuard-UI permettra de cr√©er et de g√©rer facilement les clients VPN WireGuard. Nous pourrons configurer les param√®tres de connexion et accorder ou r√©voquer les autorisations d'acc√®s.
2. **Configuration simplifi√©e** : L'interface web de WireGuard-UI simplifie la configuration du serveur VPN WireGuard. Nous pourrons facilement ajouter de nouveaux tunnels VPN et d√©finir les param√®tres r√©seau.
3. **Interface utilisateur intuitive** : WireGuard-UI est con√ßu pour offrir une exp√©rience utilisateur conviviale et intuitive. Les fonctionnalit√©s principales sont accessibles via une interface simple et bien organis√©e, permettant aux administrateurs de g√©rer efficacement le serveur VPN sans avoir √† se plonger dans des commandes complexes.

L'installation de WireGuard-UI sur la m√™me instance EC2 que notre serveur VPN WireGuard, en utilisant Docker Compose, nous permettra d'avoir une interface web pratique et facile √† utiliser pour administrer notre VPN. Cela facilitera la gestion des clients VPN et la configuration du serveur, contribuant ainsi √† une exp√©rience d'administration simplifi√©e et efficace.

![Interface web WireGuard UI - Ici on peut voir les diff√©rents clients qui ont √©t√© cr√©√©s](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-25_a_23.36.42.png)

Interface web WireGuard UI - Ici on peut voir les diff√©rents clients qui ont √©t√© cr√©√©s

### Portainer

Afin de faciliter le d√©ploiement, la gestion et la surveillance de l'ensemble des conteneurs et des piles de conteneurs de notre infrastructure, nous utiliserons le service Portainer. Portainer est une plateforme qui offre une interface graphique conviviale pour administrer les conteneurs Docker.

Pour que le service Portainer puisse fonctionner, nous devrons installer l'agent Portainer sur les serveurs o√π nos conteneurs sont ex√©cut√©s : 

- **`cloudlab_public_facing_entrypoint`**,
- **`cloudlab_public_app_projects_server`**,
- **`cloudlab_internal_server_2`**.

Cet agent transmettra les informations pertinentes au serveur central Portainer : **`cloudlab_internal_server_1`**, permettant ainsi de superviser et de g√©rer les conteneurs √† partir d'une interface utilisateur intuitive.

L'utilisation de Portainer pr√©sente plusieurs avantages :

1. **D√©ploiement simplifi√©** : Portainer facilite le d√©ploiement des conteneurs en fournissant une interface graphique conviviale. Nous pourrons cr√©er, d√©marrer, arr√™ter et supprimer des conteneurs avec quelques clics, sans avoir √† utiliser des commandes complexes.
2. **Gestion centralis√©e** : Gr√¢ce √† Portainer, nous pourrons g√©rer tous nos conteneurs √† partir d'une seule interface. Cela simplifie la gestion de l'infrastructure en permettant de surveiller les ressources, de v√©rifier les journaux, de g√©rer les images et de mettre √† jour les conteneurs, le tout depuis une seule plateforme.
3. **Surveillance des performances** : Portainer nous offre des outils de surveillance qui permettent de suivre les performances de nos conteneurs en temps r√©el. Nous pourrons visualiser les statistiques cl√©s, telles que l'utilisation des ressources (CPU, m√©moire, etc.), les d√©bits r√©seau, les temps de r√©ponse, et prendre des mesures pour optimiser les performances si n√©cessaire.

L'installation de l'agent Portainer sur nos serveurs nous permettra de b√©n√©ficier de tous les avantages de Portainer pour la gestion et la surveillance simplifi√©es de notre infrastructure de conteneurs. Nous pourrons d√©ployer, g√©rer et superviser efficacement nos conteneurs gr√¢ce √† cette interface graphique conviviale.

En r√©sum√©, l'utilisation de Portainer avec l'agent Portainer nous offre une solution pratique et intuitive pour le d√©ploiement, la gestion et la surveillance de nos conteneurs Docker. Cela facilite l'administration de l'infrastructure et permet d'optimiser les performances de nos applications bas√©es sur des conteneurs.

![Interface web portainer - Containers du serveur **`cloudlab_public_facing_entrypoint`**](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-25_a_23.47.56.png)

Interface web portainer - Containers du serveur **`cloudlab_public_facing_entrypoint`**

### Serveur DNS Pihole

Dans notre projet, nous avons mis en place un serveur DNS pour notre infrastructure, qui sera √©galement utilis√© par notre VPN pour acc√©der √† une configuration DNS personnalis√©e si n√©cessaire. Pour cela, nous utilisons Pihole, que nous avons install√© dans un conteneur Docker sur la m√™me instance EC2 que notre serveur central Portainer, c'est-√†-dire sur l'instance EC2 appel√©e **`cloudlab_internal_server_1`**.

La raison pour laquelle nous avons choisi de regrouper le serveur central Portainer et le DNS Pihole sur une instance EC2 distincte de celle du reverse proxy NGINX et du serveur VPN WireGuard, √† savoir **`cloudlab_public_facing_entrypoint`**, est de r√©duire les risques d'une d√©faillance simultan√©e de tous les services cruciaux de notre infrastructure. En les s√©parant, nous √©vitons qu'un probl√®me affecte tous ces services en m√™me temps.

Le DNS Pihole est un √©l√©ment important de notre infrastructure, car il nous permet de configurer des enregistrements DNS internes sans avoir recours aux zones h√©berg√©es priv√©es de Route 53 avec AWS. Cela peut √™tre utile pour certains projets qui n√©cessitent une gestion personnalis√©e des enregistrements DNS. 

En r√©sum√©, l'utilisation de Pihole en tant que serveur DNS dans un conteneur Docker nous offre une flexibilit√© et une personnalisation accrues pour la gestion des enregistrements DNS internes. Sa s√©paration sur une instance EC2 distincte du serveur central Portainer contribue √† la r√©silience de notre infrastructure en cas de probl√®me.

![Interface d‚Äôadministration du serveur DNS Pihole - Ici on peut voir les requ√™tes qui passent par le serveur DNS lorsqu‚Äôune personne est connect√© au VPN](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_00.17.40.png)

Interface d‚Äôadministration du serveur DNS Pihole - Ici on peut voir les requ√™tes qui passent par le serveur DNS lorsqu‚Äôune personne est connect√© au VPN

### Les deux autres serveurs : **`cloudlab_public_app_projects_server` et `cloudlab_internal_server_2`**

Dans notre infrastructure, nous utilisons deux serveurs pour h√©berger nos projets d'applications (**`cloudlab_public_app_projects_server`**) et nos serveurs de base de donn√©es (**`cloudlab_internal_server_2`**). Cette r√©partition des t√¢ches nous permet de ne pas surcharger les deux autres serveurs cruciaux de notre infrastructure, qui sont responsables du bon fonctionnement de services essentiels tels que le reverse proxy, le serveur VPN, Portainer et le serveur DNS.

De plus, nos projets d'applications et nos bases de donn√©es ne n√©cessitent pas une utilisation quotidienne. Nous avons donc la flexibilit√© de les √©teindre lorsque nous n'en avons pas besoin, ce qui nous permet d'√©conomiser de l'argent en r√©duisant les co√ªts de notre infrastructure. Lorsqu'une instance EC2 est √©teinte, nous ne payons que le stockage des donn√©es associ√©es √† cette instance. En moyenne, notre infrastructure Cloud Lab nous co√ªte entre 15 et 20‚Ç¨ par mois.

Ces deux serveurs, qui h√©bergent nos projets d'applications et nos bases de donn√©es, n'ont pas de r√¥le critique dans le fonctionnement global de notre infrastructure, contrairement aux deux autres serveurs. Vous √™tes libre de les utiliser pour h√©berger ce que vous souhaitez, en fonction de vos besoins sp√©cifiques.

### R√©sum√© de l‚Äôinfrastructure et de sa configuration

Pour r√©sumer l'infrastructure et sa configuration, voici les diff√©rents r√¥les et fonctionnalit√©s de chaque serveur :

1. **`cloudlab_public_facing_entrypoint`** : Ce serveur fait office de point d'entr√©e vers notre infrastructure depuis Internet. Il permet d'acc√©der aux applications h√©berg√©es sur notre infrastructure, notamment nos projets d'applications qui sont expos√©s publiquement via le serveur cloudlab_public_app_projects_server. De plus, il permet √©galement l'acc√®s aux applications internes via VPN, telles que les interfaces d'administration NGINX Proxy Manager, WireGuard UI, Portainer, etc. Ce serveur est donc crucial pour le bon fonctionnement de notre infrastructure.
2. **`cloudlab_public_app_projects_server`** : Ce serveur h√©berge nos projets d'applications. Il est indirectement expos√© au public via le serveur cloudlab_public_facing_entrypoint. Les applications h√©berg√©es sur ce serveur peuvent √™tre accessibles par le public gr√¢ce au reverse proxy NGINX configur√© sur le serveur public. Cependant, l'acc√®s aux applications internes se fait uniquement via VPN.
3. **`cloudlab_internal_server_1`** : Ce serveur n'est pas accessible depuis Internet, mais uniquement via VPN. Il h√©berge une partie critique de nos applications, notamment Portainer et le serveur DNS Pihole. Portainer est un outil qui facilite la gestion et le d√©ploiement des conteneurs Docker, tandis que le serveur DNS Pihole permet de personnaliser la configuration DNS interne de notre infrastructure. Ce serveur contribue au bon fonctionnement et √† l'administration de notre infrastructure.
4. **`cloudlab_internal_server_2`** : Ce serveur n'est pas accessible depuis Internet, mais uniquement via VPN. Son r√¥le principal est d'h√©berger nos serveurs de base de donn√©es. Il est responsable du stockage et de la gestion de nos donn√©es importantes. Son acc√®s est s√©curis√© via VPN pour garantir la confidentialit√© de nos bases de donn√©es.

Chaque serveur joue un r√¥le sp√©cifique dans notre infrastructure et contribue au bon fonctionnement de nos applications et services. L'utilisation de VPN nous permet de renforcer la s√©curit√© en limitant l'acc√®s aux serveurs internes, tandis que l'exposition publique se fait √† travers le serveur **`cloudlab_public_facing_entrypoint`** pour les applications destin√©es au grand public.

## Code source Ansible de la configuration

<aside>
‚ö†Ô∏è Avant de commencer, assurez-vous d'avoir install√© Ansible sur votre machine.

</aside>

Pour configurer notre infrastructure, nous utiliserons Ansible. Rendez-vous dans le dossier `üìÅ **ansible/**`.

Ce dossier contient cinq fichiers :

1. `üìÑ¬†**ansible.cfg**` : Dans ce fichier, vous devez modifier la variable du chemin de votre cl√© SSH **`private_key_file`** avec le chemin vers votre cl√© SSH. Assurez-vous √©galement de sp√©cifier le nom d'utilisateur dans la variable **`remote_user`** qui correspond √† l'utilisateur pour lequel votre cl√© SSH est configur√©e.
    
    ```bash
    private_key_file = ../ssh_keys/HomelabInfra.pem
    ```
    
2. `üìÑ¬†**hosts.ini**` : Dans ce fichier, vous devez sp√©cifier les adresses IP publiques de chaque serveur.
    - La section **`[cloudlab_public_facing_entrypoint]`** contient l'adresse IP publique du serveur cloudlab_public_facing_entrypoint.
    - La section **`[cloudlab_public_app_projects_server]`** contient l'adresse IP publique du serveur cloudlab_public_app_projects_server.
    - La section **`[cloudlab_internal_server_1]`** contient l'adresse IP publique du serveur cloudlab_internal_server_1.
    - La section **`[cloudlab_internal_server_2]`** contient l'adresse IP publique du serveur cloudlab_internal_server_2.

<aside>
‚ö†Ô∏è Notez que les adresses IP publiques des serveurs cloudlab_public_app_projects_server, cloudlab_internal_server_1 et cloudlab_internal_server_2 ne sont pas statiques et changent √† chaque red√©marrage des serveurs.

</aside>

<aside>
‚ÑπÔ∏è Vous pouvez r√©cup√©rer les adresses IP publiques en acc√©dant √† la console AWS, en recherchant EC2, en cliquant sur Instances dans la barre de navigation de gauche, puis en s√©lectionnant une instance pour afficher son adresse IP publique.

</aside>

```bash
[cloudlab_public_facing_entrypoint]
--IP publique du serveur cloudlab_public_facing_entrypoint--

[cloudlab_public_app_projects_server]
--IP publique du serveur cloudlab_public_app_projects_server--

[cloudlab_internal_server_1]
--IP publique du serveur cloudlab_internal_server_1--

[cloudlab_internal_server_2]
--IP publique du serveur cloudlab_internal_server_2--
```

- **`üìÑ¬†install-docker.yml`** : playbook ansible pour l‚Äôinstallation de docker sur tous les serveurs.
- **`üìÑ¬†install-portainer-agent.yml`** : playbook ansible pour l‚Äôinstallation des agents portainer sur les serveurs **`cloudlab_public_facing_entrypoint` , `cloudlab_public_app_projects_server`** et **`cloudlab_internal_server_2`.**
- **`üìÑ¬†install-portainer.yml`** : playbook ansible pour l‚Äôinstallation de Portainer sur le serveur **`cloudlab_internal_server_1`**.

## Configuration avec Ansible et Portainer

### Tester la connectivit√©

Pour configurer notre infrastructure avec Ansible, nous allons d'abord tester la connectivit√© entre Ansible et les serveurs en utilisant la commande suivante :

```bash
ansible all -m ping
```

Cette commande permet de v√©rifier si Ansible parvient √† se connecter correctement aux serveurs. Si Ansible ne parvient pas √† se connecter aux serveurs, assurez-vous d'utiliser la cl√© SSH appropri√©e et de sp√©cifier le bon utilisateur pour la connexion. V√©rifiez √©galement que les adresses IP des serveurs sont correctement configur√©es dans le fichier **`hosts.ini`** d'Ansible. Veuillez vous assurer d'avoir correctement configur√© le CIDR bloc 0.0.0.0/0 dans les sources autoris√©es des groupes de s√©curit√© des serveurs √† l'aide de Terraform.

Dans le contexte d'une infrastructure r√©seau AWS, les groupes de s√©curit√© jouent un r√¥le crucial dans la d√©finition des r√®gles de s√©curit√© pour les instances EC2. Lorsque vous configurez les groupes de s√©curit√© √† l'aide de Terraform, il est important de sp√©cifier les sources autoris√©es qui peuvent acc√©der aux instances EC2.

Le CIDR bloc 0.0.0.0/0 est une notation qui repr√©sente toutes les adresses IP possibles, ce qui signifie que toutes les adresses IP sont autoris√©es √† acc√©der aux instances EC2 concern√©es. Cela permet un acc√®s ouvert depuis n'importe quelle adresse IP.

V√©rifiez donc que vous avez correctement ajout√© le CIDR bloc 0.0.0.0/0 dans les sources autoris√©es des groupes de s√©curit√© de vos serveurs √† l'aide de Terraform. Cette configuration garantit que les instances EC2 sont accessibles depuis toutes les adresses IP.

Lors de l'ex√©cution de la commande, vous devriez recevoir une r√©ponse indiquant si la connexion a r√©ussi (SUCCESS) ou √©chou√© (UNREACHABLE). Cette √©tape est essentielle pour s'assurer que la configuration ult√©rieure avec Ansible se d√©roulera correctement.

### Ex√©cution des playbooks Ansible

Pour d√©ployer les configurations sur les serveurs de notre infrastructure, nous utiliserons Ansible, qui est une plateforme d'automatisation et de gestion de configuration. Nous avons pr√©par√© trois playbooks Ansible pour faciliter l'installation des composants n√©cessaires sur chaque serveur.

Le premier playbook, `üìÑ¬†**install-docker.yml**`, sera utilis√© pour installer Docker sur tous les serveurs.

Le deuxi√®me playbook, `üìÑ¬†**install-portainer-agent.yml**`, sera ex√©cut√© pour installer les agents Portainer sur les serveurs **`cloudlab_public_facing_entrypoint`**, **`cloudlab_public_app_projects_server`** et **`cloudlab_internal_server_2`**. Les agents Portainer permettent de g√©rer et de surveiller les conteneurs Docker de mani√®re centralis√©e.

Enfin, le troisi√®me playbook, `üìÑ¬†**install-portainer.yml**`, sera utilis√© pour installer Portainer sur le serveur **`cloudlab_internal_server_1`**.

Pour ex√©cuter un playbook Ansible, ouvrez votre terminal et assurez-vous d'√™tre dans le dossier **`ansible/`**. Ensuite, ex√©cutez la commande suivante en rempla√ßant **`[nom_du_playbook_√†_ex√©cuter]`** par le nom du playbook que vous souhaitez ex√©cuter :

```bash
ansible-playbook [nom_du_playbook_√†_ex√©cuter] -vv
```

Cette commande lancera l'ex√©cution du playbook sp√©cifi√© et vous fournira des informations d√©taill√©es sur les t√¢ches en cours d'ex√©cution.

Assurez-vous de suivre cette proc√©dure pour chaque playbook afin d'installer correctement les composants n√©cessaires sur les serveurs.

### Configuration de Portainer

<aside>
‚ÑπÔ∏è Lien vers la documentation de Portainer si n√©cessaire : [https://docs.portainer.io](https://docs.portainer.io/)

</aside>

<aside>
‚ö†Ô∏è Cette infrastructure n√©cessite la version gratuite de Portainer Business Edition qui permet de g√©rer jusqu‚Äô√† 5 noeuds (serveurs). Cela n√©cessite de se cr√©er un compte pour ce faire : [https://www.portainer.io/take-5](https://www.portainer.io/take-5).

</aside>

Maintenant, nous allons passer √† la configuration de Portainer. Pour commencer, ouvrez votre navigateur et saisissez l'adresse IP publique du serveur **`cloudlab_internal_server_1`**, suivie du port 9000. Par exemple, si l'adresse IP publique est 192.168.0.1, vous devez acc√©der √† l'URL **`http://192.168.0.1:9000`**.

Une fois que vous avez configur√© Portainer, vous pouvez ajouter les environnements des autres serveurs √† votre interface. Pour ce faire, suivez ces √©tapes :

1. Connectez-vous √† Portainer √† l'aide de vos informations d'identification.
2. Dans la barre de navigation de gauche, cliquez sur "Settings" (Param√®tres).
3. S√©lectionnez "Environments" (Environnements).
4. Cliquez sur le bouton "+ Add environment" (Ajouter un environnement).
5. Dans la fen√™tre pop-up, s√©lectionnez "Docker Standalone".
6. Cliquez sur "Start wizard" (D√©marrer l'assistant).

Maintenant, vous allez ajouter les informations sp√©cifiques pour chaque serveur :

- Pour le serveur **`cloudlab_public_facing_entrypoint`**, saisissez son adresse IP priv√©e suivi du port 9001. Par exemple, si l'adresse IP priv√©e est 10.0.1.11, entrez "10.0.1.11:9001".
- Pour le serveur **`cloudlab_public_app_projects_server`**, utilisez son adresse IP priv√©e suivie du port 9001 de la m√™me mani√®re que pr√©c√©demment.
- Pour le serveur **`cloudlab_internal_server_2`**, utilisez √©galement son adresse IP priv√©e suivie du port 9001.

Assurez-vous de remplir le champ "Name" (Nom) avec le nom que vous souhaitez donner √† chaque environnement. Par exemple, vous pouvez utiliser "cloudlab_public_facing_entrypoint" pour le premier serveur.

R√©p√©tez ces √©tapes pour tous les serveurs que vous souhaitez ajouter √† Portainer.

Une fois que vous avez ajout√© tous les environnements, vous pourrez acc√©der √† chacun d'eux depuis l'interface de Portainer. Cela vous permettra de visualiser et de g√©rer les conteneurs Docker sur chaque serveur de mani√®re centralis√©e, ce qui facilite l'administration et la surveillance de votre infrastructure.

![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 20.20.21.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_20.20.21.png)

![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 20.22.03.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_20.22.03.png)

![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 20.23.09.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_20.23.09.png)

### D√©ploiement des autres applications avec Portainer

Une fois que tous les serveurs sont connect√©s √† Portainer, nous pouvons proc√©der au d√©ploiement des services sur nos serveurs en utilisant Portainer. Dans notre cas, nous allons commencer par configurer le serveur **`cloudlab_public_facing_entrypoint`** en d√©ployant le service NGINX Proxy Manager.

Voici les √©tapes √† suivre :

1. Acc√©dez au dossier **`üìÅ¬†docker/`** dans votre syst√®me.
2. Ouvrez le fichier **`üìÑ¬†docker-compose-nginx-proxy-manager.yml`**.
3. Copiez le contenu du fichier.
4. Connectez-vous √† l'interface d'administration de Portainer.
5. Dans la barre de navigation de gauche, s√©lectionnez "Stacks".
6. Cliquez sur le bouton "+ Add stack" pour cr√©er une nouvelle stack.
7. Donnez un nom √† la stack, par exemple **`nginx-proxy-manager`**.
8. Collez le contenu du fichier **`üìÑ¬†docker-compose-nginx-proxy-manager.yml`** dans l'√©diteur web de Portainer.
9. Cliquez sur le bouton "Deploy the stack" en bas de la page pour d√©ployer la stack.

![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 20.47.49.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_20.47.49.png)

![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 20.49.42.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_20.49.42.png)

![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 20.49.47.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_20.49.47.png)

Une fois que vous avez d√©ploy√© la stack, Portainer se chargera de cr√©er et de configurer les conteneurs n√©cessaires pour ex√©cuter NGINX Proxy Manager sur le serveur **`cloudlab_public_facing_entrypoint`**. Ce service agira comme un routeur et un point d'entr√©e pour notre infrastructure, facilitant la gestion des connexions entrantes et sortantes.

Une fois le d√©ploiement du service NGINX Proxy Manager termin√©, vous pouvez acc√©der √† son interface d'administration via votre navigateur. Pour ce faire, entrez l'adresse IP du serveur suivi du port 81 dans la barre d'URL.

Une fois sur la page de connexion de l'interface d'administration de NGINX Proxy Manager, utilisez les identifiants par d√©faut suivants : **`admin@example.com`** comme adresse e-mail et **`changeme`** comme mot de passe. Apr√®s vous √™tre connect√©, il est recommand√© de modifier ces identifiants par d√©faut en choisissant une adresse e-mail et un mot de passe personnalis√©s.

Une fois connect√© et apr√®s avoir chang√© vos identifiants, vous pourrez commencer √† ajouter vos Proxy Hosts. Les Proxy Hosts vous permettent de configurer les connexions entrantes et de rediriger le trafic vers les applications sp√©cifiques h√©berg√©es sur votre infrastructure. Vous pouvez ajouter autant de Proxy Hosts que n√©cessaire en sp√©cifiant les d√©tails tels que l'adresse IP et le port du serveur cible, ainsi que les param√®tres de routage appropri√©s.

Voici un exemple :

![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 21.02.14 (2).png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_21.02.14_(2).png)

![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 21.03.57.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_21.03.57.png)

![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 21.04.02.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_21.04.02.png)

Une fois que vous avez configur√© vos Proxy Hosts dans NGINX Proxy Manager, nous allons maintenant proc√©der au d√©ploiement du serveur VPN WireGuard et de son interface web d'administration, WireGuard-UI. Nous allons effectuer cette √©tape sur le m√™me serveur, **`cloudlab_public_facing_entrypoint`**.

Pour configurer le d√©ploiement de WireGuard, suivez ces √©tapes simples :

1. Acc√©dez au dossier **`üìÅ docker/`** o√π se trouvent les fichiers n√©cessaires pour la configuration.
2. Ouvrez le fichier **`üìÑ docker-compose-wireguard.yml`** avec un √©diteur de texte.
3. Recherchez la variable d'environnement **`SERVERURL`** dans la configuration de WireGuard.
4. Remplacez la valeur de la variable **`SERVERURL`** par l'adresse IP publique du serveur o√π WireGuard sera d√©ploy√© ou par le nom de domaine que vous avez sp√©cifiquement d√©di√© √† WireGuard. Assurez-vous que cette adresse est accessible depuis Internet.
5. Copiez le contenu du fichier **`üìÑ docker-compose-wireguard.yml`**.

Ces √©tapes permettent de configurer correctement WireGuard en sp√©cifiant l'adresse IP publique ou le nom de domaine pour le serveur. Cette information est essentielle pour que les clients VPN puissent se connecter au bon serveur WireGuard.

Il est important de noter que l'utilisation d'un nom de domaine peut faciliter la gestion et l'acc√®s √† votre infrastructure, car vous pouvez associer un nom facilement m√©morisable √† votre serveur au lieu d'utiliser une adresse IP.

Effectuez la m√™me proc√©dure que pour le d√©ploiement de NGINX Proxy Manager.

Une fois que vous avez d√©ploy√© WireGuard et WireGuard-UI, connectez-vous en SSH au serveur **`cloudlab_public_facing_entrypoint`**.

Pour cela, ouvrez votre terminal et ex√©cutez la commande suivante :

```bash
ssh -i [chemin_vers_votre_cl√©_ssh] [nom]@[ip]
# Exemple : ssh -i ssh_keys/HomelabInfra.pem admin@13.37.148.174
```

Une fois connect√© en SSH, nous allons v√©rifier la valeur actuelle de la variable de noyau **`net.ipv4.ip_forward`**. Cette variable permet au syst√®me d'envoyer des paquets IP entre les diff√©rentes interfaces r√©seau, ce qui est n√©cessaire pour le fonctionnement du serveur VPN.

Ex√©cutez la commande suivante :

```bash
sudo sysctl net.ipv4.ip_forward
```

Si le r√©sultat retourn√© n'est pas **`net.ipv4.ip_forward = 1`**, cela signifie que la redirection des paquets IP n'est pas activ√©e. Dans ce cas, suivez les √©tapes suivantes pour activer la redirection :

1. Ex√©cutez la commande suivante pour ouvrir le fichier de configuration **`sysctl.conf`** avec l'√©diteur de texte Nano :
    
    ```bash
    sudo nano /etc/sysctl.conf
    ```
    
    Ce fichier contient les param√®tres de configuration du syst√®me. Vous pouvez d√©commenter la ligne **`net.ipv4.ip_forward=1`** en supprimant le symbole **`#`** au d√©but de la ligne.
    
    ![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 22.15.14.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_22.15.14.png)
    
2. Enregistrez et fermez le fichier **`sysctl.conf`**. Pour charger les nouvelles configurations, ex√©cutez la commande suivante :
    
    ```bash
    sudo sysctl -p
    ```
    
    Cela permet d'appliquer les modifications du fichier **`sysctl.conf`** sans red√©marrer le syst√®me.
    
3. V√©rifiez √† nouveau la valeur de la variable **`net.ipv4.ip_forward`** en ex√©cutant la commande suivante :
    
    ```bash
    sudo sysctl net.ipv4.ip_forward
    ```
    
    Assurez-vous que le r√©sultat affiche **`net.ipv4.ip_forward = 1`**.
    

Ces commandes sont importantes pour la configuration d'un serveur VPN parce qu'elles permettent au serveur de relayer correctement le trafic entre le client VPN et le reste d'Internet.

Apr√®s avoir configur√© les √©tapes pr√©c√©dentes, vous pouvez acc√©der √† l'interface web d'administration du serveur VPN, WireGuard-UI. Pour cela, ouvrez votre navigateur et entrez l'adresse IP publique du serveur suivi du port 5000.

Une fois sur l'interface, vous serez invit√© √† vous connecter. Les identifiants par d√©faut sont **`admin`** pour le nom d'utilisateur et **`admin`** pour le mot de passe. Il est recommand√© de modifier ces identifiants par la suite pour des raisons de s√©curit√©.

Une fois connect√©, vous pourrez configurer le serveur WireGuard. Dans la barre de navigation √† gauche, cliquez sur ‚ÄúWireguard Server‚Äù. Dans la section ‚ÄúServer Interface Addresses‚Äù, vous pouvez sp√©cifier le bloc CIDR du r√©seau pour vos clients. Par exemple, vous pouvez choisir le bloc CIDR **`192.168.2.1/14`**.

Apr√®s avoir saisi le bloc CIDR, cliquez sur ‚ÄúSave‚Äù pour enregistrer les modifications.

![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 22.34.32.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_22.34.32.png)

Lors de la configuration de la plage d'adresses IP pour les clients du serveur WireGuard, il est important de choisir une plage qui r√©pond √† vos besoins. Cependant, il est essentiel de comprendre que cette plage d'adresses IP est uniquement valide √† l'int√©rieur du conteneur Docker de WireGuard.

Lorsque les clients interagissent avec d'autres services sur le m√™me serveur, mais qui se trouvent dans leurs propres conteneurs, tels que NGINX Proxy Manager, le client sera identifi√© par l'adresse IP du conteneur Docker, par exemple **`172.28.0.2`**. En revanche, lorsque les clients interagissent avec des services h√©berg√©s sur un autre serveur de l'infrastructure, ils seront identifi√©s par l'adresse IP priv√©e du serveur sur lequel se trouve le serveur VPN, c'est-√†-dire **`cloudlab_public_facing_entrypoint`** avec l'adresse IP **`10.0.1.11`**.

Il est important de prendre en compte cette distinction lors de la configuration et de l'utilisation du serveur VPN WireGuard. Les adresses IP attribu√©es aux clients peuvent varier en fonction du contexte dans lequel ils interagissent avec les services de l'infrastructure.

Pour continuer la configuration du VPN, rendez-vous dans la barre de navigation √† gauche de l'interface d'administration WireGuard-UI et cliquez sur ‚ÄúGlobal Settings‚Äù. Dans la section ‚ÄúDNS Servers‚Äù, saisissez l'adresse IP priv√©e du serveur **`cloudlab_internal_server_1`**, qui est **`10.0.2.11`**. Ce serveur h√©berge notre serveur DNS Pihole. Ensuite, cliquez sur ‚ÄúSave‚Äù pour enregistrer les modifications.

Cette √©tape est importante car elle permet au serveur VPN WireGuard de rediriger les requ√™tes DNS vers le serveur DNS Pihole. Ainsi, toutes les requ√™tes DNS effectu√©es par les clients du VPN passeront par le serveur Pihole.

![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 22.49.27.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_22.49.27.png)

Maintenant que le serveur VPN WireGuard est configur√©, vous pouvez cr√©er des clients pour permettre √† d'autres utilisateurs de se connecter au VPN. Pour ce faire, rendez-vous dans la barre de navigation √† gauche de l'interface d'administration WireGuard-UI et cliquez sur "Wireguard Clients". Ensuite, cliquez sur le bouton "+ New Client" en haut √† droite de la page.

Donnez un nom au client, cela peut √™tre un identifiant ou un nom descriptif. Vous pouvez √©galement renseigner son adresse e-mail si vous le souhaitez. La cr√©ation d'une adresse IP pour le client est automatique, vous n'avez pas √† vous en pr√©occuper.

Dans le champ "Allowed IPs", vous devez sp√©cifier les plages d'adresses IP auxquelles le client est autoris√© √† acc√©der. Pour cela, vous pouvez utiliser les blocs CIDR suivants : `**172.0.0.0/8**`, **`10.0.0.0/16`**. Ces plages d'adresses permettront au client de communiquer avec d'autres services du r√©seau interne.

Ensuite, ajoutez l'adresse IP publique du serveur **`cloudlab_public_facing_entrypoint`** suivie de **`/32`**. Par exemple, si l'adresse IP publique est `**13.37.148.174**`, vous pouvez entrer **`13.37.148.174/32`**. Cela permettra au client d'acc√©der sp√©cifiquement √† ce serveur.

Une fois que vous avez renseign√© toutes les informations n√©cessaires, cliquez sur "Save" pour cr√©er le client.

Une fois que vous avez cr√©√© le client et finalis√© la configuration du serveur VPN WireGuard, vous devez appliquer les modifications pour les rendre effectives. Pour cela, vous verrez appara√Ætre un bouton "Apply Config" en haut √† gauche de la page de l'interface d'administration WireGuard-UI. Cliquez sur ce bouton pour appliquer la configuration.

![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 23.03.36.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_23.03.36.png)

Maintenant que vous avez cr√©√© le client VPN dans WireGuard-UI, vous pouvez t√©l√©charger le fichier de configuration correspondant. Cliquez simplement sur le bouton ‚Äúdownload‚Äù pour r√©cup√©rer le fichier.

<aside>
‚ÑπÔ∏è Pour utiliser ce fichier de configuration sur votre machine, vous devez d'abord t√©l√©charger et installer le client VPN WireGuard depuis le site officiel (**[https://www.wireguard.com/install/](https://www.wireguard.com/install/)**).

</aside>

Une fois install√©, ouvrez le logiciel client WireGuard sur votre machine et importez le fichier de configuration que vous avez t√©l√©charg√©.

L'importation du fichier de configuration permettra √† votre machine d'√©tablir une connexion VPN avec le serveur WireGuard, en utilisant les param√®tres sp√©cifiques du client que vous avez cr√©√©. Cela vous permettra d'acc√©der aux ressources de l'infrastructure r√©seau depuis votre machine de mani√®re s√©curis√©e.

Apr√®s avoir configur√© votre client VPN WireGuard et √©tabli la connexion avec le serveur, vous pourriez constater que l'acc√®s √† Internet ne fonctionne pas. Cela est d√ª au fait que nous n'avons pas encore d√©ploy√© le serveur DNS Pihole sur le serveur cloudlab_internal_server_1, qui est utilis√© dans notre configuration VPN.

La prochaine √©tape consistera donc √† d√©ployer le serveur DNS Pihole. Cela nous permettra d'avoir une r√©solution DNS personnalis√©e et efficace pour les requ√™tes Internet effectu√©es via le VPN. Gr√¢ce √† Pihole, nous pourrons √©galement mettre en place des filtres et des r√®gles de blocage pour am√©liorer la s√©curit√© et la confidentialit√© lors de la navigation.

Le d√©ploiement du serveur DNS Pihole sera abord√© plus en d√©tail dans les √©tapes suivantes, afin de compl√©ter la configuration de notre infrastructure r√©seau et de permettre un acc√®s s√©curis√© et contr√¥l√© √† Internet via le VPN.

Pour le d√©ploiement du serveur DNS Pihole, il suffit de suivre la m√™me proc√©dure que pour les autres services sauf que cette fois le d√©ploiement doit ce faire sur le serveur **`cloudlab_internal_server_1`** veillez donc bien √† s√©lectionner le bon serveur sur l‚Äôinterface d‚Äôadministration de Portainer.  Rendez-vous dans le dossier **`üìÅ¬†docker/`** o√π vous trouverez les fichiers n√©cessaires. Ouvrez le fichier **`üìÑ¬†docker-compose-pihole.yml`** et copiez son contenu.

Effectuez la m√™me proc√©dure que pour le d√©ploiement de NGINX Proxy Manager, WireGuard et WireGuard-UI.

Une fois que vous avez d√©ploy√© Pihole, rendez-vous sur son interface d‚Äôadministration web, ouvrez votre navigateur et saisissez l‚ÄôIP publique du serveur **`cloudlab_internal_server_1`** suivi du port 85 avec le chemin **`/admin/`** le mot de passe par d√©faut est **`admin`.** Vous devriez avoir l‚Äôinterface suivante :

![Capture d‚ÄôeÃÅcran 2023-06-26 aÃÄ 23.37.05.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-26_a_23.37.05.png)

Pour d√©ployer le serveur DNS Pihole, suivez la m√™me proc√©dure que pour les autres services en utilisant l'interface d'administration de Portainer. Cependant, cette fois-ci, assurez-vous de s√©lectionner le serveur **`cloudlab_internal_server_1`** lors du d√©ploiement.

Pour cela, rendez-vous dans le dossier : **`üìÅ docker/`** o√π vous trouverez les fichiers n√©cessaires. Ouvrez le fichier **`üìÑ¬†docker-compose-pihole.yml`** et copiez son contenu.

Ensuite, suivez la m√™me proc√©dure que pour le d√©ploiement pr√©c√©dent en collant le contenu du fichier dans le web editor de l'interface d'administration de Portainer et en cliquant sur le bouton Deploy the stack pour lancer le d√©ploiement de Pihole.

Une fois que Pihole est d√©ploy√© avec succ√®s, vous pouvez acc√©der √† son interface d'administration web. Ouvrez votre navigateur et saisissez l'IP publique du serveur **`cloudlab_internal_server_1`** suivi du port 85 et ajoutez le chemin **`/admin/`**. Par exemple, **`http://<IP_publique>:85/admin/`**.

Vous serez redirig√© vers l'interface d'administration de Pihole o√π vous pourrez configurer les param√®tres avanc√©s, les filtres et les r√®gles de blocage en fonction de vos besoins sp√©cifiques. Le mot de passe par d√©faut pour acc√©der √† l'interface est **`admin`**.

Il reste un dernier service √† d√©ployer cette fois sur le serveur **`cloudlab_public_facing_entrypoint`**, il s‚Äôagit de GoAccess. 

Pour d√©ployer le service GoAccess sur le serveur **`cloudlab_public_facing_entrypoint`**, suivez la m√™me proc√©dure que pour les autres d√©ploiements en utilisant l'interface d'administration de Portainer.

Pour configurer correctement le d√©ploiement de GoAccess, suivez ces √©tapes simples :

1. Acc√©dez au dossier `**üìÅ docker/**` o√π se trouvent les fichiers n√©cessaires pour la configuration.
2. Ouvrez le fichier **`üìÑ docker-compose-goaccess.yml`** avec un √©diteur de texte.
3. Recherchez le mapping de volume pour d√©finir le chemin d'acc√®s aux logs de NGINX Proxy Manager.
4. Remplacez **`/your/path/to/logs`** par le chemin r√©el sur le serveur o√π NGINX Proxy Manager est ex√©cut√©. Pour trouver ce chemin, acc√©dez √† l'interface d'administration de Portainer et s√©lectionnez le serveur **`cloudlab_public_facing_entrypoint`**. Acc√©dez aux informations du conteneur NGINX Proxy Manager, puis dans la section "Volumes" en bas de la page, copiez le chemin qui contient le dossier "data". Par exemple, le chemin pourrait √™tre **`/data/compose/5/data/logs`**.
5. Remplacez **`/your/path/to/`** dans le mapping de volume du fichier de configuration de GoAccess par le chemin que vous avez copi√©. Par cons√©quent, le nouveau chemin sera **`/data/compose/5/data/logs`**.
6. Copiez l'int√©gralit√© du contenu du fichier **`üìÑ docker-compose-goaccess.yml`**.

Ensuite, collez le contenu du fichier dans le web editor de l'interface d'administration de Portainer et cliquez sur le bouton Deploy the stack pour lancer le d√©ploiement de GoAccess.

Ce service permettra de g√©n√©rer des rapports et des statistiques sur l'utilisation et l'acc√®s √† vos applications h√©berg√©es sur le serveur **`cloudlab_public_facing_entrypoint`**. Vous pourrez ainsi obtenir des informations d√©taill√©es sur les visiteurs, les pages consult√©es, etc.

Une fois que GoAccess est d√©ploy√© avec succ√®s, vous pouvez acc√©der √† son interface via votre navigateur. Ouvrez votre navigateur et entrez l'IP publique du serveur cloudlab_public_facing_entrypoint suivi du port sp√©cifi√© dans le fichier de configuration (par exemple, **`http://<IP_publique>:7880`**).

Sur la page de connexion de GoAccess, utilisez les identifiants d'acc√®s par d√©faut : **`admin`** pour le nom d'utilisateur et **`admin`** pour le mot de passe. Vous pouvez choisir de les modifier ult√©rieurement pour des raisons de s√©curit√©.

Si vous souhaitez modifier les identifiants d'acc√®s par d√©faut, vous pouvez le faire en modifiant la configuration de la stack de GoAccess sur Portainer. Une fois les modifications effectu√©es, assurez-vous de red√©ployer la stack pour que les nouvelles configurations prennent effet.

Maintenant que tous les services essentiels de notre infrastructure sont d√©ploy√©s, nous devons terminer la configuration de NGINX Proxy Manager. Pour cela, acc√©dez √† son interface d'administration via votre navigateur web.

Dans la barre de navigation de NGINX Proxy Manager, recherchez l'option "Access Lists" et cliquez dessus. Ensuite, cliquez sur le bouton "Add access list" pour cr√©er une nouvelle liste d'acc√®s.

Une fen√™tre modale s'ouvrira o√π vous pourrez donner un nom √† votre liste d'acc√®s. Choisissez un nom significatif qui refl√®te le but de cette liste.

Une fois que vous avez cr√©√© la liste d'acc√®s, rendez-vous dans l'onglet "Access" pour configurer les r√®gles. Ajoutez des r√®gles "Allow" pour les plages d'adresses IP suivantes : `**172.0.0.0/8**`, **`10.0.0.0/16`** et l'adresse IP publique du serveur cloudlab_public_facing_entrypoint suivie de **`/32`**. Par exemple, si l'adresse IP publique est **`13.37.148.174`**, vous pouvez entrer **`13.37.148.174/32`**.

Ces r√®gles permettent d'autoriser l'acc√®s aux adresses IP sp√©cifi√©es √† travers NGINX Proxy Manager. Cela garantit que seules les adresses IP autoris√©es peuvent acc√©der aux services expos√©s par l'infrastructure.

![Capture d‚ÄôeÃÅcran 2023-06-27 aÃÄ 00.16.02.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-27_a_00.16.02.png)

Assurez-vous de sauvegarder les modifications une fois que vous avez configur√© les r√®gles d'acc√®s. Cette √©tape est cruciale pour renforcer la s√©curit√© de votre infrastructure et √©viter tout acc√®s non autoris√©.

Il est recommand√© de r√©guli√®rement revoir et mettre √† jour les listes d'acc√®s en fonction des besoins de s√©curit√© de votre infrastructure.

Maintenant que vous avez cr√©√© votre liste d'acc√®s, vous pouvez l'appliquer √† vos Proxy Hosts pour contr√¥ler l'acc√®s √† vos applications. Lorsque vous cr√©ez ou modifiez un Proxy Host dans NGINX Proxy Manager, vous verrez une option appel√©e "Access List".

Cliquez sur la liste d√©roulante et s√©lectionnez l'access list que vous avez pr√©c√©demment cr√©√©e. Cela permettra d'associer cette access list sp√©cifique au Proxy Host en question.

En appliquant l'access list √† un Proxy Host, vous d√©finissez les r√®gles d'acc√®s qui seront utilis√©es pour contr√¥ler qui peut acc√©der √† cette application sp√©cifique. Par exemple, si vous avez d√©fini une r√®gle "Allow" pour une certaine plage d'adresses IP dans votre access list, seules les adresses IP de cette plage seront autoris√©es √† acc√©der √† l'application associ√©e au Proxy Host.

Cette fonctionnalit√© de contr√¥le d'acc√®s bas√©e sur les access lists vous permet de restreindre l'acc√®s √† vos applications √† des utilisateurs sp√©cifiques ou √† des plages d'adresses IP approuv√©es. Cela renforce la s√©curit√© de votre infrastructure en r√©duisant les risques d'acc√®s non autoris√©.

![Capture d‚ÄôeÃÅcran 2023-06-27 aÃÄ 00.24.09.png](Rapport%20-%20Mise%20en%20place%20d%E2%80%99une%20architecture%20et%20d%E2%80%99un%2027f99427bf774bc3b1879f05c0628e29/Capture_decran_2023-06-27_a_00.24.09.png)

Maintenant que vous avez configur√© votre access list, vous pouvez proc√©der √† la cr√©ation de vos Proxy Hosts pour les diff√©rents services de votre infrastructure. Assurez-vous de ne pas oublier cette √©tape si vous ne l'avez pas encore r√©alis√©e.

Lorsque vous cr√©ez un Proxy Host pour un service sp√©cifique, vous avez la possibilit√© d'appliquer l'access list que vous avez pr√©c√©demment cr√©√©e. Cela vous permet de contr√¥ler l'acc√®s √† ce service en fonction des r√®gles d√©finies dans l'access list.

Cependant, il est important de noter que si vous utilisez un Proxy Host pour le domaine associ√© √† votre VPN WireGuard, il est essentiel de ne pas appliquer l'access list √† ce Proxy Host. Si vous le faites, cela risque de bloquer l'acc√®s au VPN et vous ne pourrez plus vous connecter.

La raison en est que le VPN WireGuard agit comme une passerelle d'acc√®s √† votre infrastructure depuis des emplacements externes. Si vous appliquez une access list restrictive √† ce Proxy Host, cela emp√™chera les connexions VPN l√©gitimes et rendra impossible l'√©tablissement de connexions s√©curis√©es.

Par cons√©quent, veillez √† faire preuve de prudence lors de la configuration des Proxy Hosts et √† ne pas appliquer l'access list au Proxy Host associ√© √† votre VPN WireGuard. Cela garantira que vous pouvez toujours acc√©der √† votre infrastructure de mani√®re s√©curis√©e via le VPN.

Concernant les instances EC2 **`cloudlab_public_app_projects_server`** et **`cloudlab_internal_server_2`**, elles ont √©t√© ajout√©es √† notre infrastructure dans le cadre de nos projets sp√©cifiques, notamment nos projets universitaires. Ces serveurs ne jouent pas de r√¥les critiques dans le fonctionnement global de l'infrastructure, mais ils offrent une flexibilit√© suppl√©mentaire pour le d√©ploiement de services suppl√©mentaires.

Ces deux instances offrent donc une flexibilit√© suppl√©mentaire en nous permettant de d√©ployer des services et d'ex√©cuter des projets sp√©cifiques √† notre environnement. Cependant, il est important de noter que ces serveurs ne sont pas essentiels au fonctionnement de base de notre infrastructure et peuvent √™tre adapt√©s en fonction de nos besoins √©volutifs.

# √âtape 3 : Modification de l‚Äôinfrastructure provisioning

Au cours de la configuration de notre infrastructure, nous avons utilis√© le bloc CIDR **`0.0.0.0/0`** dans les groupes de s√©curit√© pour faciliter la mise en place de l'architecture. Cependant, maintenant que nous avons termin√© la configuration, il est important de prendre des mesures de s√©curit√© suppl√©mentaires en supprimant ce bloc CIDR des autorisations accord√©es par les groupes de s√©curit√©.

Pour effectuer cette modification, nous allons acc√©der au dossier **`üìÅ terraform/`** et ouvrir les fichiers **`üìÑ3a1_subnet_configuration.tf`** et **`üìÑ3b1_subnet_configuration.tf`**. Dans ces fichiers, nous trouverons les sections concernant les groupes de s√©curit√©. Nous devrons supprimer la mention du bloc CIDR **`0.0.0.0/0`** des autorisations de chaque groupe de s√©curit√©.

Une fois que nous avons effectu√© ces modifications, nous pouvons ex√©cuter les commandes Terraform suivantes :

- **`terraform plan`** : Cette commande nous permettra de voir les modifications qui seront appliqu√©es √† notre infrastructure.
- **`terraform apply`** : En ex√©cutant cette commande, les modifications seront appliqu√©es aux groupes de s√©curit√© de notre infrastructure.

Ces √©tapes garantiront que nous avons pris les mesures de s√©curit√© appropri√©es en restreignant l'acc√®s √† notre infrastructure uniquement aux adresses IP sp√©cifiques autoris√©es. Cela renforce la s√©curit√© de notre environnement et r√©duit les risques potentiels li√©s √† des acc√®s non autoris√©s.

# Conclusion

En conclusion, ce rapport met en √©vidence notre travail sur la conception et le d√©ploiement d'une architecture et d'une infrastructure r√©seau sur AWS. Nous avons r√©ussi √† cr√©er un environnement cloud solide, s√©curis√© et √©volutif, permettant le d√©ploiement et la gestion efficace de diff√©rents services pour un faible budget.

Au cours de ce projet, nous avons utilis√© des technologies telles que Docker, Terraform, Ansible et les services AWS pour cr√©er une infrastructure robuste. Nous avons d√©ploy√© des services essentiels tels que NGINX Proxy Manager, WireGuard, Pihole et GoAccess, en suivant des proc√©dures d√©taill√©es et en veillant √† la bonne configuration de chaque composant.

La s√©curit√© a √©t√© une pr√©occupation majeure tout au long du processus de d√©ploiement. Nous avons mis en place des mesures de s√©curit√© telles que la configuration des groupes de s√©curit√©, l'application d'access lists et la restriction des acc√®s non autoris√©s. Cela garantit que notre infrastructure est prot√©g√©e contre les menaces potentielles.

Ce projet nous a permis de mieux comprendre les avantages et les d√©fis li√©s √† la mise en place d'une architecture et d'une infrastructure r√©seau sur AWS. Nous avons acquis une exp√©rience pr√©cieuse dans l'utilisation des outils et des services cloud, ainsi que dans la configuration et la gestion des diff√©rentes composantes de l'infrastructure.

En conclusion, notre projet a abouti √† la cr√©ation d'une architecture et d'une infrastructure r√©seau AWS performantes et s√©curis√©es. Nous sommes convaincus que notre approche et les connaissances acquises seront b√©n√©fiques pour d'autres personnes souhaitant d√©velopper leur propre infrastructure cloud.